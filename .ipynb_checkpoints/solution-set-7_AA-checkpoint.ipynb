{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intensive Module in Machine Learning\n",
    "# Problem set 7: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are looking at the pdf/html version of this document, start by running the command `jupyter notebook` to launch an interactive notebook and then navigate to the correct folder and open this file `problem-set-7.ipynb`. Import your default packages for manipulating data and plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Backpropagation by hand (difficult)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you really want to understand neural networks, once in your life, you have to have impolemented backpropagation by hand. You might take this exercise home."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the logistic regression example from last week usin the Spam data. View the model as a one-layer neural network. Implement a NeuralNetwork class in python which stores the wights and biases in form of numpy arrays. Your class should have methods such as `fit` and `predict`. For the fit method implement backpropagation by hand to obtain the gradients and then run gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0     1     2    3     4     5     6     7     8     9   ...    47    48  \\\n",
      "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...   0.0  0.00   \n",
      "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...   0.0  0.01   \n",
      "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...   0.0  0.00   \n",
      "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...   0.0  0.00   \n",
      "5  0.00  0.00  0.00  0.0  1.85  0.00  0.00  1.85  0.00  0.00  ...   0.0  0.00   \n",
      "\n",
      "      49   50     51     52    53     54   55    56  \n",
      "0  0.000  0.0  0.778  0.000  0.00  3.756   61   278  \n",
      "2  0.143  0.0  0.276  0.184  0.01  9.821  485  2259  \n",
      "3  0.137  0.0  0.137  0.000  0.00  3.537   40   191  \n",
      "4  0.135  0.0  0.135  0.000  0.00  3.537   40   191  \n",
      "5  0.223  0.0  0.000  0.000  0.00  3.000   15    54  \n",
      "\n",
      "[5 rows x 57 columns]\n",
      "0    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "5    1\n",
      "Name: 57, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data2 = pd.read_csv('Spam_Data.txt', sep=\",\", header=None)\n",
    "\n",
    "#Split data into training and testing data\n",
    "mask = np.random.rand(len(data2)) < 0.8\n",
    "train = data2[mask]\n",
    "test = data2[~mask]\n",
    "\n",
    "#Split training data into training and cross validation data\n",
    "mask = np.random.rand(len(train)) < 0.8\n",
    "cvset = train[~mask]\n",
    "core_train = train[mask]\n",
    "\n",
    "X2 = train.loc[:, range(0,57)]\n",
    "y2 = train.loc[:, 57]\n",
    "X2_core = core_train.loc[:, range(0,57)]\n",
    "y2_core = core_train.loc[:, 57]\n",
    "X2_cv = cvset.loc[:, range(0,57)]\n",
    "y2_cv = cvset.loc[:, 57]\n",
    "X2_test = test.loc[:, range(0,57)]\n",
    "y2_test = test.loc[:, 57]\n",
    "\n",
    "print(X2.head())\n",
    "print(y2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Neural Networks with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the above model using tensorflow. To understand the functionalitites better, please implement explicitly the placeholders for all the tensors as well as the loop iterating over the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ablaas/anaconda3/envs/tensorflow/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Users/ablaas/anaconda3/envs/tensorflow/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits:  Tensor(\"fully_connected/Relu:0\", shape=(?, 2), dtype=float32)\n",
      "loss:  Tensor(\"Mean:0\", shape=(), dtype=float32)\n",
      "predicted_labels:  Tensor(\"ArgMax:0\", shape=(?,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#Initialise random seed for reproducability\n",
    "random_state = 2000\n",
    "np.random.seed(random_state)\n",
    "tf.set_random_seed(random_state)\n",
    "\n",
    "# Initialize placeholders \n",
    "x = tf.placeholder(dtype = tf.float32, shape = [None, 57])\n",
    "y = tf.placeholder(dtype = tf.int32, shape = [None])\n",
    "# Have 1 fully connected layer \n",
    "logits = tf.contrib.layers.fully_connected(x, 2)\n",
    "# Define a loss function\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits = logits))\n",
    "# Define an optimizer \n",
    "train_op = tf.train.GradientDescentOptimizer(learning_rate=0.001).minimize(loss)\n",
    "# Convert logits to label indexes\n",
    "correct_pred = tf.argmax(logits, 1)\n",
    "# Define an accuracy metric\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "print(\"logits: \", logits)\n",
    "print(\"loss: \", loss)\n",
    "print(\"predicted_labels: \", correct_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "for i in range(1001):\n",
    "        _, accuracy_val = sess.run([train_op, accuracy], feed_dict={x: X2, y: y2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.918\n"
     ]
    }
   ],
   "source": [
    "# Run predictions against the full test set.\n",
    "predicted = sess.run([correct_pred], feed_dict={x: X2_test})[0]\n",
    "# Calculate correct matches \n",
    "match_count = sum([int(y == y_) for y, y_ in zip(y2_test, predicted)])\n",
    "# Calculate the accuracy\n",
    "accuracy = match_count / len(y2_test)\n",
    "# Print the accuracy\n",
    "print(\"Accuracy: {:.3f}\".format(accuracy))\n",
    "\n",
    "# Close the session\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 High-level APIs for Tensorflow: Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, we can use high-level APIs such as Keras to do the above. Now building the network becomes a one-liner and you can use `model.fit(X)` and `model.predict(X,y)` similar to Sklean. Build the above neural network using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 58        \n",
      "=================================================================\n",
      "Total params: 58\n",
      "Trainable params: 58\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=1, activation='sigmoid',input_dim=57))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd',metrics=[\"accuracy\"])\n",
    "# summarize the model\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early_Stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "history = model.fit(X2.values, y2, epochs=100, batch_size=32, verbose=0)\n",
    "#tl,vl = historyBNN.history['loss'], historyBNN.history['val_loss']           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.9218585005279831\n"
     ]
    }
   ],
   "source": [
    "ypred = model.predict(X2_test.values, batch_size=32)\n",
    "ypred[ypred > 0.5] = 1\n",
    "ypred[ypred <= 0.5] = 0\n",
    "test_acc = np.mean(np.equal(ypred, np.expand_dims(y2_test,axis=1)))\n",
    "print('Test accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Practice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Spam classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above you already implemented a one-layer neural network in Keras for the Spam data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Now add one hidden-layer to the model. Optimise your hyper-parameters and report on the best model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "256\n",
      "512\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.core import Lambda\n",
    "from keras import backend as K\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "hidden_units = [128,256,512,1024]\n",
    "dropout_ratios = [0.2,0.4,0.6]\n",
    "\n",
    "scores = np.zeros(len(hidden_units))\n",
    "dropout_value = np.zeros(len(hidden_units))\n",
    "        \n",
    "for ii in range(0,len(hidden_units)):\n",
    "    best_score = 0.0\n",
    "    for d in dropout_ratios:\n",
    "        model = Sequential()\n",
    "        model.add(Dense(units=hidden_units[ii], activation='relu', input_dim=57))\n",
    "        model.add(Lambda(lambda x: K.dropout(x,level=d)))\n",
    "        model.add(Dense(units=1))\n",
    "        sgd = SGD(lr=0.01, momentum=0.9, nesterov=True)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='sgd',metrics=[\"accuracy\"])\n",
    "        history = model.fit(X2_core.values, y2_core, epochs=1000, batch_size=32, verbose=0)\n",
    "        ypred = model.predict(X2_cv.values, batch_size=32)\n",
    "        ypred[ypred > 0.5] = 1\n",
    "        ypred[ypred <= 0.5] = 0\n",
    "        valid_acc = np.mean(np.equal(ypred, np.expand_dims(y2_cv,axis=1)))\n",
    "        if valid_acc > best_score:\n",
    "            best_score = valid_acc\n",
    "            scores[ii] = valid_acc\n",
    "            dropout_value[ii] = d\n",
    "    print(hidden_units[ii])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61538462 0.61538462 0.61538462 0.61538462]\n"
     ]
    }
   ],
   "source": [
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Try different architectures, e.g. with 2 hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_31 (Dense)             (None, 512)               29696     \n",
      "_________________________________________________________________\n",
      "lambda_16 (Lambda)           (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "lambda_17 (Lambda)           (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 161,281\n",
      "Trainable params: 161,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(units=512, activation='relu',input_dim=57))\n",
    "model.add(Lambda(lambda x: K.dropout(x,level=0.2)))\n",
    "model.add(Dense(units=256, activation='relu'))\n",
    "model.add(Lambda(lambda x: K.dropout(x,level=0.2)))\n",
    "model.add(Dense(units=1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd',metrics=[\"accuracy\"])\n",
    "# summarize the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Continue practising ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at another example from last week, like the heart data, and try to build the classification model using a neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
